import logging

import numpy as np
import xarray as xr
import pandas as pd

import nnja_ai

from eagle.tools.data import open_anemoi_inference_dataset, open_forecast_zarr_dataset
from eagle.tools.metrics import postprocess

logger = logging.getLogger("eagle.tools")

GRAVITY = 9.80665

UNIT_CONVERSIONS = {
    "gp_to_gph": lambda x: x / GRAVITY,
}

DATASET_REGISTRY = {
    "conv-adpupa-NC002001": {
        "t": {"obs_var": "TMDB", "obs_qc_var": "QMAT"},
        "gh": {"obs_var": "GP10", "obs_qc_var": "QMGP", "unit_conversion": "gp_to_gph"},
        "u": {"obs_wspd_var": "WSPD", "obs_wdir_var": "WDIR", "obs_qc_var": "QMWN"},
        "v": {"obs_wspd_var": "WSPD", "obs_wdir_var": "WDIR", "obs_qc_var": "QMWN"},
    },
    "conv-adpsfc-NC000001": {
        "t2m": {"obs_var": "TMPSQ1.TMDB", "obs_qc_var": "TMPSQ1.QMAT"},
        "u10": {"obs_wspd_var": "WNDSQ1.WSPD", "obs_wdir_var": "WNDSQ1.WDIR", "obs_qc_var": "WNDSQ1.QMWN"},
        "v10": {"obs_wspd_var": "WNDSQ1.WSPD", "obs_wdir_var": "WNDSQ1.WDIR", "obs_qc_var": "WNDSQ1.QMWN"},
    },
    "conv-adpsfc-NC000002": {
        "t2m": {"obs_var": "TMPSQ1.TMDB", "obs_qc_var": "TMPSQ1.QMAT"},
        "u10": {"obs_wspd_var": "WNDSQ1.WSPD", "obs_wdir_var": "WNDSQ1.WDIR", "obs_qc_var": "WNDSQ1.QMWN"},
        "v10": {"obs_wspd_var": "WNDSQ1.WSPD", "obs_wdir_var": "WNDSQ1.WDIR", "obs_qc_var": "WNDSQ1.QMWN"},
    },
    "conv-adpsfc-NC000007": {
        "t2m": {"obs_var": "MTRTMP.TMDB", "obs_qc_var": "MTRTMP.QMAT"},
        "u10": {"obs_wspd_var": "MTRWND.WSPD", "obs_wdir_var": "MTRWND.WDIR", "obs_qc_var": "MTRWND.QMWN"},
        "v10": {"obs_wspd_var": "MTRWND.WSPD", "obs_wdir_var": "MTRWND.WDIR", "obs_qc_var": "MTRWND.QMWN"},
    },
    "conv-adpsfc-NC000101": {
        "t2m": {"obs_var": "TEMHUMDA.TMDB", "obs_qc_var": "QMAT"},
        "u10": {"obs_wspd_var": "BSYWND1.WSPD", "obs_wdir_var": "BSYWND1.WDIR", "obs_qc_var": "QMWN"},
        "v10": {"obs_wspd_var": "BSYWND1.WSPD", "obs_wdir_var": "BSYWND1.WDIR", "obs_qc_var": "QMWN"},
    },
}

WIND_VARIABLES = {
    "u":   {"group": "uv",   "component": "u"},
    "v":   {"group": "uv",   "component": "v"},
    "u10": {"group": "uv10", "component": "u"},
    "v10": {"group": "uv10", "component": "v"},
}

DEFAULT_VARIABLES = ["t", "gh", "t2m", "u", "v", "u10", "v10"]
DEFAULT_LEVELS = [500, 850]


def _is_upper_air(base_name):
    """A variable is upper-air if it appears in any adpupa dataset."""
    return any(
        base_name in reg and "adpupa" in ds_name
        for ds_name, reg in DATASET_REGISTRY.items()
    )


def build_variable_map(config):
    """Expand config into a flat map keyed by forecast variable name.

    Reads a flat ``variables`` list and a ``levels`` list from config.
    Upper-air variables (those appearing in any adpupa dataset) are expanded
    across all levels; surface variables get a single entry with level=None.

    Each entry uses level-specific obs column names so that multiple levels
    can coexist in the same DataFrame without collisions.

    Returns:
        dict keyed by forecast variable name (e.g. "t_850", "u_850", "v10").
    """
    all_registry_vars = set()
    for reg in DATASET_REGISTRY.values():
        all_registry_vars.update(reg.keys())

    variables = config.get("variables", DEFAULT_VARIABLES)
    levels = config.get("levels", DEFAULT_LEVELS)
    variable_map = {}

    for base_name in variables:
        if base_name not in all_registry_vars:
            raise ValueError(
                f"Variable '{base_name}' is not available in any dataset. "
                f"Available variables: {sorted(all_registry_vars)}"
            )

        # Look up unit_conversion from the first registry entry that has this variable
        conversion = None
        for reg in DATASET_REGISTRY.values():
            if base_name in reg:
                conversion = reg[base_name].get("unit_conversion", None)
                break

        upper_air = _is_upper_air(base_name)

        if upper_air and levels:
            for level in levels:
                forecast_var = f"{base_name}_{level}"
                entry = {
                    "base_name": base_name,
                    "level": level,
                    "obs_col": f"obs_{base_name}_{level}",
                    "obs_qc_col": f"obs_qc_{base_name}_{level}",
                    "unit_conversion": conversion,
                }
                if base_name in WIND_VARIABLES:
                    wind_info = WIND_VARIABLES[base_name]
                    group = wind_info["group"]
                    entry["obs_wspd_col"] = f"obs_wspd_{group}_{level}"
                    entry["obs_wdir_col"] = f"obs_wdir_{group}_{level}"
                    entry["obs_qc_col"] = f"obs_qc_{group}_{level}"
                    entry["wind_component"] = wind_info["component"]
                variable_map[forecast_var] = entry
        else:
            # Surface variable — no levels
            entry = {
                "base_name": base_name,
                "level": None,
                "obs_col": f"obs_{base_name}",
                "obs_qc_col": f"obs_qc_{base_name}",
                "unit_conversion": conversion,
            }
            if base_name in WIND_VARIABLES:
                wind_info = WIND_VARIABLES[base_name]
                group = wind_info["group"]
                entry["obs_wspd_col"] = f"obs_wspd_{group}"
                entry["obs_wdir_col"] = f"obs_wdir_{group}"
                entry["obs_qc_col"] = f"obs_qc_{group}"
                entry["wind_component"] = wind_info["component"]
            variable_map[base_name] = entry

    return variable_map


def load_all_observations(time_range, variable_map):
    """Load observations from all datasets in DATASET_REGISTRY.

    For each dataset, determines which user-requested variables it supports,
    builds the real column names, loads from nnja_ai, renames to standardized
    names, and concatenates all DataFrames.

    Args:
        time_range: (start, end) tuple of pd.Timestamps for time selection.
        variable_map: Output of build_variable_map.

    Returns:
        pd.DataFrame with LAT, LON, OBS_TIMESTAMP, and standardized
        obs/QC columns (obs_{var}, obs_qc_{var}).
    """
    dc = nnja_ai.DataCatalog()
    all_frames = []

    for dataset_name, registry in DATASET_REGISTRY.items():
        # Build a unified rename_map (real col -> standardized col) for all
        # requested variables supported by this dataset.
        rename_map = {}
        has_any = False
        for forecast_var, vinfo in variable_map.items():
            base_name = vinfo["base_name"]
            if base_name not in registry:
                continue
            has_any = True
            reg = registry[base_name]
            level = vinfo["level"]

            if "obs_wspd_col" in vinfo:
                # Wind variable: map WSPD, WDIR, and QC columns
                if level is not None:
                    prlc_suffix = f"PRLC{level * 100}"
                    real_wspd = f"{reg['obs_wspd_var']}_{prlc_suffix}"
                    real_wdir = f"{reg['obs_wdir_var']}_{prlc_suffix}"
                    real_qc = f"{reg['obs_qc_var']}_{prlc_suffix}"
                else:
                    real_wspd = reg["obs_wspd_var"]
                    real_wdir = reg["obs_wdir_var"]
                    real_qc = reg["obs_qc_var"]
                rename_map[real_wspd] = vinfo["obs_wspd_col"]
                rename_map[real_wdir] = vinfo["obs_wdir_col"]
                rename_map[real_qc] = vinfo["obs_qc_col"]
            else:
                # Direct variable
                if level is not None:
                    prlc_suffix = f"PRLC{level * 100}"
                    real_obs_col = f"{reg['obs_var']}_{prlc_suffix}"
                    real_qc_col = f"{reg['obs_qc_var']}_{prlc_suffix}"
                else:
                    real_obs_col = reg["obs_var"]
                    real_qc_col = reg["obs_qc_var"]
                rename_map[real_obs_col] = vinfo["obs_col"]
                rename_map[real_qc_col] = vinfo["obs_qc_col"]

        if not has_any:
            continue

        # Build column list for this dataset
        columns = ["LAT", "LON", "OBS_TIMESTAMP"] + list(rename_map.keys())
        columns = list(dict.fromkeys(columns))  # deduplicate, preserve order

        ds = dc[dataset_name]
        try:
            subds = ds.sel(
                time=slice(str(time_range[0]), str(time_range[1])),
                variables=columns,
            )
            obs_df = subds.load_dataset()
        except nnja_ai.exceptions.EmptyTimeSubsetError:
            logger.warning(f"No observations in {dataset_name} for {time_range[0]} to {time_range[1]}")
            continue

        # Rename real column names to standardized names
        obs_df = obs_df.rename(columns=rename_map)

        logger.info(f"Loaded {len(obs_df)} observations from {dataset_name}")
        all_frames.append(obs_df)

    if all_frames:
        result = pd.concat(all_frames, ignore_index=True)
        logger.info(f"Total observations across all datasets: {len(result)}")
    else:
        # Build empty DataFrame with expected columns
        std_columns = ["LAT", "LON", "OBS_TIMESTAMP"]
        for vinfo in variable_map.values():
            std_columns.append(vinfo["obs_col"])
            std_columns.append(vinfo["obs_qc_col"])
            if "obs_wspd_col" in vinfo:
                std_columns.append(vinfo["obs_wspd_col"])
                std_columns.append(vinfo["obs_wdir_col"])
        std_columns = list(dict.fromkeys(std_columns))
        result = pd.DataFrame(columns=std_columns)
        logger.warning("No observations loaded from any dataset")
    return result


def apply_qc_filter(obs_df, variable_map, max_qc_value=2):
    """Apply per-variable QC filtering.

    Masks obs values to NaN where QC is non-NaN AND > max_qc_value.
    NaN QC means not flagged -> keep. 0-2 = good -> keep. 3+ = suspect/rejected -> mask.

    For wind variables, masks the WSPD and WDIR source columns (obs_col
    doesn't exist yet — it is derived later).
    """
    for forecast_var, vinfo in variable_map.items():
        qc_col = vinfo["obs_qc_col"]
        if qc_col not in obs_df.columns:
            continue

        qc_vals = obs_df[qc_col]
        bad = qc_vals.notna() & (qc_vals > max_qc_value)
        n_rejected = bad.sum()

        if "obs_wspd_col" in vinfo:
            # Wind variable: mask WSPD and WDIR source columns
            cols_to_mask = [vinfo["obs_wspd_col"], vinfo["obs_wdir_col"]]
            for col in cols_to_mask:
                if col in obs_df.columns:
                    if n_rejected > 0:
                        logger.info(f"QC filter: masking {n_rejected} obs in {col} for {forecast_var} (QC > {max_qc_value})")
                    obs_df.loc[bad, col] = np.nan
        else:
            obs_col = vinfo["obs_col"]
            if obs_col in obs_df.columns:
                if n_rejected > 0:
                    logger.info(f"QC filter: masking {n_rejected} obs for {forecast_var} (QC > {max_qc_value})")
                obs_df.loc[bad, obs_col] = np.nan
    return obs_df


def derive_wind_components(obs_df, variable_map):
    """Derive u/v wind components from WSPD/WDIR columns.

    For each wind variable in variable_map, computes:
        u = -wspd * sin(wdir_rad)
        v = -wspd * cos(wdir_rad)

    Tracks already-derived (wspd_col, wdir_col) pairs to avoid duplicate work
    when u and v share the same source columns.
    """
    derived = set()
    for forecast_var, vinfo in variable_map.items():
        if "obs_wspd_col" not in vinfo:
            continue
        wspd_col = vinfo["obs_wspd_col"]
        wdir_col = vinfo["obs_wdir_col"]
        obs_col = vinfo["obs_col"]
        component = vinfo["wind_component"]

        key = (wspd_col, wdir_col, component)
        if key in derived:
            continue
        derived.add(key)

        if wspd_col not in obs_df.columns or wdir_col not in obs_df.columns:
            continue

        wdir_rad = np.deg2rad(obs_df[wdir_col])
        wspd = obs_df[wspd_col]
        if component == "u":
            obs_df[obs_col] = -wspd * np.sin(wdir_rad)
        else:
            obs_df[obs_col] = -wspd * np.cos(wdir_rad)
        logger.info(f"Derived {obs_col} from {wspd_col}/{wdir_col}")
    return obs_df


def convert_obs_units(obs_df, variable_map):
    """Apply unit conversions to observation columns as specified in variable map."""
    for forecast_var, vinfo in variable_map.items():
        conversion = vinfo["unit_conversion"]
        if conversion is not None:
            obs_col = vinfo["obs_col"]
            if obs_col in obs_df.columns:
                obs_df[obs_col] = UNIT_CONVERSIONS[conversion](obs_df[obs_col])
    return obs_df


def align_obs_to_forecast_times(obs_df, forecast_valid_times, window):
    """Match observations to forecast valid times within a temporal window.

    Args:
        obs_df: DataFrame with OBS_TIMESTAMP column.
        forecast_valid_times: Array of forecast valid times (np.datetime64).
        window: pd.Timedelta for +/- matching window.

    Returns:
        dict mapping pd.Timestamp -> DataFrame of matched observations.
    """
    aligned = {}
    obs_times = obs_df["OBS_TIMESTAMP"]

    # Remove timezone info if present, to match forecast times (tz-naive)
    if hasattr(obs_times.dtype, "tz") and obs_times.dtype.tz is not None:
        obs_times = obs_times.dt.tz_localize(None)

    for vt in forecast_valid_times:
        vtimestamp = pd.Timestamp(vt)
        mask = (obs_times >= vtimestamp - window) & (obs_times < vtimestamp + window)
        matched = obs_df.loc[mask]
        if len(matched) > 0:
            aligned[vtimestamp] = matched
    return aligned


def compute_obs_metrics(forecast_values, obs_values):
    """Compute verification metrics between forecast and observation values.

    Args:
        forecast_values: Array of forecast values at obs locations.
        obs_values: Array of observation values.

    Returns:
        dict with rmse, mae, bias, count.
    """
    valid = np.isfinite(forecast_values) & np.isfinite(obs_values)
    n = valid.sum()
    if n == 0:
        result = {"rmse": np.nan, "mae": np.nan, "bias": np.nan, "count": 0}
    else:
        f = forecast_values[valid]
        o = obs_values[valid]
        diff = f - o
        result = {
            "rmse": float(np.sqrt(np.mean(diff**2))),
            "mae": float(np.mean(np.abs(diff))),
            "bias": float(np.mean(diff)),
            "count": int(n),
        }
    xds = xr.Dataset(result)
    xds = xds.expand_dims({"time": [forecast_values["time"].values]})
#    if "level" in forecast_values.coords:
#        xds = xds.expand_dims({"level": [forecast_values["level"].values]})
    return xds


def main(config):
    """Verify forecasts against observations.

    See ``eagle-tools obs-metrics --help`` or cli.py for help.
    """

    topo = config["topo"]

    # Build variable map
    variable_map = build_variable_map(config)
    forecast_var_names = list(variable_map.keys())
    # Extract unique base variable names and levels for loading the forecast
    base_var_names = list({vinfo["base_name"] for vinfo in variable_map.values()})
    levels = sorted({v["level"] for v in variable_map.values() if v["level"] is not None})
    logger.info(f"Variables to verify: {forecast_var_names}")

    # Config options
    model_type = config.get("model_type")
    lam_index = config.get("lam_index", None)
    lead_time = config["lead_time"]
    temporal_window = pd.Timedelta(config.get("temporal_window", "3h"))
    max_qc_value = config.get("max_qc_value", 2)

    # does the user want to evaluate on a different grid?
    # this doesn't include regridding the nested -> global resolution
    target_regrid_kwargs = config.get("target_regrid_kwargs", None)
    forecast_regrid_kwargs = config.get("forecast_regrid_kwargs", None)
    do_any_regridding = (target_regrid_kwargs is not None) or \
            ((forecast_regrid_kwargs is not None) and (model_type != "nested-global"))
    if do_any_regridding:
        raise NotImplementedError

    if model_type == "nested-global":
        forecast_regrid_kwargs["target_grid_path"] = prepare_regrid_target_mask(
            anemoi_reference_dataset_kwargs=config["anemoi_reference_dataset_kwargs"],
            horizontal_regrid_kwargs=forecast_regrid_kwargs,
        )

    # Generate initialization dates
    dates = pd.date_range(config["start_date"], config["end_date"], freq=config["freq"])
    n_dates = len(dates)
    n_batches = int(np.ceil(n_dates / topo.size))

    container = {"rmse": [], "mae": [], "bias": [], "count": []}

    logger.info(f"Observation Verification")
    logger.info(f"Datasets: {list(DATASET_REGISTRY.keys())}")
    logger.info(f"Temporal window: +/- {temporal_window}")
    logger.info(f"Max QC value: {max_qc_value}")
    logger.info(f"Initial Conditions:\n{dates}")

    for batch_idx in range(n_batches):

        date_idx = (batch_idx * topo.size) + topo.rank
        if date_idx + 1 > n_dates:
            break

        try:
            t0 = dates[date_idx]
        except Exception:
            logger.error(f"Error getting this date: {date_idx} / {n_dates}")
            raise

        st0 = t0.strftime("%Y-%m-%dT%H")
        logger.info(f"Processing {st0}")

        # Load forecast using base variable names and levels
        if config.get("from_anemoi", True):
            fname = f"{config['forecast_path']}/{st0}.{lead_time}h.nc"
            fds = open_anemoi_inference_dataset(
                fname,
                model_type=model_type,
                lam_index=lam_index,
                trim_edge=config.get("trim_forecast_edge", None),
                vars_of_interest=base_var_names,
                levels=levels,
                load=True,
                lcc_info=config.get("lcc_info", None),
                horizontal_regrid_kwargs=forecast_regrid_kwargs if model_type == "nested-global" else None,
                reshape_cell_to_2d=True,
            )
        else:
            fds = open_forecast_zarr_dataset(
                config["forecast_path"],
                t0=t0,
                vars_of_interest=base_var_names,
                levels=levels,
                load=True,
                lcc_info=config.get("lcc_info", None),
            )

        # Pad in longitude
        if "global" in model_type:
            dlon = fds["longitude"].diff("longitude").values[0]
            fds = fds.pad({"longitude": 1}, mode="wrap")
            plon = np.concatenate([
                [fds["longitude"][1].values - dlon],
                fds["longitude"].values[1:-1],
                [fds["longitude"][-2].values + dlon],
            ])
            fds["longitude"] = plon

        # Get forecast valid times
        logger.info(f"Opened fds\n{fds}")
        forecast_valid_times = fds["time"].values
        logger.info(f"vtimes: {forecast_valid_times}")

        # Load observations for the full valid time range (padded by window)
        time_start = pd.Timestamp(forecast_valid_times[0]) - temporal_window
        time_end = pd.Timestamp(forecast_valid_times[-1]) + temporal_window
        obs_df = load_all_observations((time_start, time_end), variable_map)

        # QC filter and unit conversion
        obs_df = apply_qc_filter(obs_df, variable_map, max_qc_value=max_qc_value)
        obs_df = convert_obs_units(obs_df, variable_map)
        obs_df = derive_wind_components(obs_df, variable_map)

        # Convert obs longitudes to 0-360 to match forecast convention
        obs_df["LON"] = obs_df["LON"] % 360

        # Align observations to forecast valid times
        aligned = align_obs_to_forecast_times(obs_df, forecast_valid_times, temporal_window)

        # Compute metrics per forecast valid time
        container_per_ic = {metric: {varname: [] for varname in forecast_var_names} for metric in container.keys()}

        for vtime in forecast_valid_times:
            vtimestamp = pd.Timestamp(vtime)

            # Handle case where we don't have obs
            if vtimestamp not in aligned:
                for varname in forecast_var_names:
                    for metric in container.keys():
                        fillvalue = 0 if metric == "count" else np.nan
                        fillds = xr.DataArray(fillvalue, coords={"time": [vtime]}, dims=("time",))
                        container_per_ic[metric][varname].append(fillds)
                continue

            # TODO: maybe do this conversion earlier?
            matched_obs = aligned[vtimestamp].to_xarray()

            # Interp to obs locations and compute metrics
            interpolated = fds.sel(time=vtime).interp({
                "longitude": matched_obs["LON"],
                "latitude": matched_obs["LAT"],
            })
            for varname, vinfo in variable_map.items():
                fvals = interpolated[vinfo["base_name"]]
                if "level" in fvals.dims:
                    fvals = fvals.sel(level=vinfo["level"])

                result = compute_obs_metrics(fvals, matched_obs[vinfo["obs_col"]])
                for metric in container.keys():
                    container_per_ic[metric][varname].append(result[metric])

        # Assemble into xr.Dataset, grouping upper-air variables by base
        # name with a level dimension
        for metric, thedata in container_per_ic.items():
            base_groups = {}
            for varname, vals in thedata.items():
                vinfo = variable_map[varname]
                bn = vinfo["base_name"]
                level = vinfo["level"]
                time_concat = xr.concat(vals, dim="time")
                if bn not in base_groups:
                    base_groups[bn] = {}
                base_groups[bn][level] = time_concat

            data_vars = {}
            for bn, level_dict in base_groups.items():
                if None in level_dict:
                    # Surface variable — no level dimension
                    data_vars[bn] = level_dict[None]
                else:
                    # Upper-air variable — stack levels
                    level_arrays = []
                    for lvl in sorted(level_dict.keys()):
                        arr = level_dict[lvl].expand_dims({"level": [lvl]})
                        level_arrays.append(arr)
                    data_vars[bn] = xr.concat(level_arrays, dim="level")

            this_metric_ds = postprocess(xr.Dataset(data_vars))
            container[metric].append(this_metric_ds)

        logger.info(f"Done with {st0}")

    logger.info("Done Computing Observation Verification Metrics")

    logger.info("Gathering Results on Root Process")
    for name in container.keys():
        container[name] = topo.gather(container[name])

    if topo.is_root:
        for name in container:
            c = container[name]
            if config["use_mpi"]:
                c = [xds for sublist in c for xds in sublist]
            c = sorted(c, key=lambda xds: xds.coords["t0"])
            container[name] = xr.concat(c, dim="t0")

        for metric, xds in container.items():
            fname = f"{config['output_path']}/obs.conv.{metric}.nc"
            xds.to_netcdf(fname)
            logger.info(f"Stored result: {fname}")
        logger.info("Done Storing Observation Verification Metrics")
